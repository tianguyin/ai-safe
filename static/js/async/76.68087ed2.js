"use strict";(self.webpackChunkaisafe=self.webpackChunkaisafe||[]).push([["76"],{593:function(n,e,r){r.r(e),r.d(e,{default:()=>d});var s=r(2676),l=r(453);function i(n){let e=Object.assign({h1:"h1",a:"a",h2:"h2",p:"p",pre:"pre",code:"code",img:"img",blockquote:"blockquote",strong:"strong",h4:"h4",ul:"ul",li:"li"},(0,l.ah)(),n.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(e.h1,{id:"典型架构卷积神经网络的参数恢复",children:["典型架构卷积神经网络的参数恢复",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#典型架构卷积神经网络的参数恢复",children:"#"})]}),"\n",(0,s.jsxs)(e.h2,{id:"为什么要进行参数恢复",children:["为什么要进行参数恢复",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#为什么要进行参数恢复",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"对于典型架构卷积神经网络，我们必须要获取其cnn参数才能正常运行其权重文件，这是后续攻击的基础。"}),"\n",(0,s.jsx)(e.p,{children:"class cnn就像是一个接口，告诉框架该模型怎么去调用。"}),"\n",(0,s.jsxs)(e.h2,{id:"获取cnn模型类名",children:["获取CNN（模型类名）",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#获取cnn模型类名",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"例题 2025 软件安全赛（天津赛区）ez_sight ai部分 这里采用这道题目，是因为这是一个已经训练好了的典型架构卷积神经网络的权重模型（我懒的自己训练一个了），本篇文章也是建立在典型架构卷积神经网络的基础上进行参数恢复"}),"\n",(0,s.jsx)(e.p,{children:"当然，当时这道题目给了一个公告文档 不过我这里并不打算通过这个文档解题，这里我们通过恢复参数的方式进行解题"}),"\n",(0,s.jsx)(e.p,{children:"目前我们没有任何已知信息，我们只有一个password.pt的权重文件"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\r\n\r\n# 加载模型\r\nmodel = torch.load('password.pt', map_location='cpu', weights_only=False)\r\n\r\ndef analyze_cnn_structure(model):\r\n    print(f\"模型类名: {model.__class__.__name__}\")\r\n\r\nanalyze_cnn_structure(model)\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{src:"/image-20250705020557203.png",alt:"image-20250705020557203"})}),"\n",(0,s.jsx)(e.p,{children:"从上方代码中我们可以看到，由于没有从报错中我们，我们可以获取模型的cnn类名 SimpleCNN"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"注意如果你的cnn类名称为SimpleCNN 那你的class 一定为class SimpleCNN"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"注意 ，典型架构卷积神经网络 的 CNN一般长下面这个样子"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class CNN(nn.Module):\r\n    def __init__(self):\r\n        super(CNN, self).__init__()  # 初始化父类\r\n        <你的参数>\r\n\r\n    def forward(self, x):\r\n       <你的参数>\n"})}),"\n",(0,s.jsx)(e.p,{children:"那么，我们可以通过 构造一个无用的SimpleCNN 来尝试调用该权重文件"}),"\n",(0,s.jsx)(e.p,{children:"如果成功，就可以通过该方法获取SimpleCNN的模型参数"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass SimpleCNN(nn.Module):\r\n    def __init__(self):\r\n        super(SimpleCNN, self).__init__()\r\n        pass\r\n\r\n    def forward(self, x):\r\n        pass\r\n\r\n\r\n# 加载模型\r\nmodel = torch.load(\'password.pt\', map_location=\'cpu\', weights_only=False)\r\n\r\n\r\ndef analyze_cnn_structure(model):\r\n    print(f"模型类名: {model.__class__.__name__}")\r\n    print()\r\n\r\n    # 分析每一层，按类型分类\r\n    conv_layers = []\r\n    fc_layers = []\r\n    other_layers = []\r\n\r\n    # 遍历模型的所有模块\r\n    for name, module in model.named_modules():\r\n        if isinstance(module, nn.Conv2d):\r\n            conv_layers.append((name, module))\r\n        elif isinstance(module, nn.Linear):\r\n            fc_layers.append((name, module))\r\n        elif isinstance(module, (nn.BatchNorm2d, nn.ReLU, nn.MaxPool2d, nn.Dropout)):\r\n            other_layers.append((name, module))\r\n\r\n    # 打印卷积层信息\r\n    print("卷积层:")\r\n    for name, layer in conv_layers:\r\n        print(f"  {name}: {layer}")\r\n        print(f"    输入通道数: {layer.in_channels}")\r\n        print(f"    输出通道数: {layer.out_channels}")\r\n        print(f"    卷积核大小: {layer.kernel_size}")\r\n        print(f"    步长: {layer.stride}")\r\n        print(f"    填充: {layer.padding}")\r\n        print()\r\n\r\n    # 打印全连接层信息\r\n    print("全连接层:")\r\n    for name, layer in fc_layers:\r\n        print(f"  {name}: {layer}")\r\n        print(f"    输入特征数: {layer.in_features}")\r\n        print(f"    输出特征数: {layer.out_features}")\r\n        print()\r\n\r\n    # 打印其他层信息\r\n    print("其他层:")\r\n    for name, layer in other_layers:\r\n        print(f"  {name}: {layer}")\r\n\r\n    # 显示参数统计\r\n    print("\\n参数统计:")\r\n    total_params = 0\r\n    for name, param in model.named_parameters():\r\n        param_count = param.numel()\r\n        total_params += param_count\r\n        print(f"  {name}: {param.shape} -> {param_count} 个参数")\r\n\r\n# 调用分析函数\r\nanalyze_cnn_structure(model)\n'})}),"\n",(0,s.jsx)(e.p,{children:"通过遍历model，来获取所有的参数"}),"\n",(0,s.jsx)(e.p,{children:"这里直接附上截图，也可以自己跑一下"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{src:"/image-20250705023550381.png",alt:"image-20250705023550381"})}),"\n",(0,s.jsx)(e.p,{children:"我们可以通过Pycharm的注释看到更清楚一点"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{src:"/image-20250705024040400.png",alt:"image-20250705024040400"})}),"\n",(0,s.jsxs)(e.h2,{id:"推断数据流动路径",children:[(0,s.jsx)(e.strong,{children:"推断数据流动路径"}),(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#推断数据流动路径",children:"#"})]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"def forward(self, x):\r\n    x = self.pool(F.relu(self.conv1(x)))\r\n    x = self.pool(F.relu(self.conv2(x)))\r\n    x = x.view(x.size(0), -1)\r\n    x = F.relu(self.fc1(x))\r\n    x = self.fc2(x)\r\n    return x\n"})}),"\n",(0,s.jsxs)(e.p,{children:["基于组件信息，可以按以下步骤重构 ",(0,s.jsx)(e.code,{children:"forward"})," 逻辑："]}),"\n",(0,s.jsxs)(e.h4,{id:"1卷积部分",children:[(0,s.jsx)(e.strong,{children:"（1）卷积部分"}),(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#1卷积部分",children:"#"})]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["典型 CNN 的卷积部分遵循 ",(0,s.jsx)(e.strong,{children:"卷积 → 激活 → 池化"})," 的顺序。"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"每一层的输出通道数需与下一层的输入通道数匹配。"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"self.conv1(x)"}),"：对输入进行第一次卷积操作。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"F.relu(...)"}),"：应用 ReLU 激活函数引入非线性。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"self.pool(...)"}),"：使用最大池化降维，保留主要特征。"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"重复上述步骤完成第二个卷积块。"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.h4,{id:"2展平操作",children:[(0,s.jsx)(e.strong,{children:"（2）展平操作"}),(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#2展平操作",children:"#"})]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["卷积输出需要通过 ",(0,s.jsx)(e.code,{children:"view()"})," 或 ",(0,s.jsx)(e.code,{children:"flatten()"})," 转换为一维向量，才能输入全连接层。"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["展平后的维度需与 ",(0,s.jsx)(e.code,{children:"fc1"})," 的输入特征数一致（本例中为 ",(0,s.jsx)(e.code,{children:"32 * 8 * 8"}),"）。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"保留批次维度"}),"：",(0,s.jsx)(e.code,{children:"x.size(0)"})," 表示批次大小（batch size），即当前批次中有多少个样本。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"展平特征维度"}),"：",(0,s.jsx)(e.code,{children:"-1"}),' 是一个特殊参数，表示 "自动计算该维度的大小"，PyTorch 会根据张量的总元素数和已知维度（',(0,s.jsx)(e.code,{children:"x.size(0)"}),"）来推断这个维度的值。"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.h4,{id:"3全连接部分",children:[(0,s.jsx)(e.strong,{children:"（3）全连接部分"}),(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#3全连接部分",children:"#"})]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"通常包含非线性激活（如 ReLU），但最后一层分类任务一般不加激活（直接接 softmax）。"}),"\n",(0,s.jsx)(e.p,{children:"x = F.relu(self.fc1(x))    # 第一个全连接层 + ReLU 激活 x = self.fc2(x)            # 第二个全连接层（输出层，无激活）"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"现在我们来看公告中的用例守则"}),"\n",(0,s.jsx)(e.p,{children:"请注意公司内部AI模型的使用规范：\r\n1.除最后一层外与池化层外其他隐藏层输出均需要通过激活函数\r\n2.至少需要通过两次池化层\r\n3.注意隐藏之间输出数据格式的匹配，必要时对数据张量进行重塑\r\n4.为保证模型准确性，输入图片应转换为灰度图"}),"\n",(0,s.jsx)(e.p,{children:"3和4不在讨论范围之内"}),"\n",(0,s.jsx)(e.p,{children:"1.除最后一层外与池化层外其他隐藏层输出均需要通过激活函数"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"self.conv1(x)"}),"：对输入进行第一次卷积操作。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"F.relu(...)"}),"：应用 ReLU 激活函数引入非线性。"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"self.pool(...)"}),"：使用最大池化降维，保留主要特征。"]}),"\n",(0,s.jsx)(e.p,{children:"完成了"}),"\n",(0,s.jsx)(e.p,{children:"2.至少需要通过两次池化层"}),"\n",(0,s.jsx)(e.p,{children:"重复上述步骤完成第二个卷积块。"}),"\n",(0,s.jsx)(e.p,{children:"发现，两个用例都已经遵守了。"}),"\n",(0,s.jsx)(e.p,{children:"通过该方式，我们可以简单的在黑盒状态下调用权重模型"})]})}function c(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}=Object.assign({},(0,l.ah)(),n.components);return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(i,{...n})}):i(n)}let d=c;c.__RSPRESS_PAGE_META={},c.__RSPRESS_PAGE_META["start%2Fclass_cnn.md"]={toc:[{text:"为什么要进行参数恢复",id:"为什么要进行参数恢复",depth:2},{text:"获取CNN（模型类名）",id:"获取cnn模型类名",depth:2},{text:"**推断数据流动路径**",id:"推断数据流动路径",depth:2},{text:"**（1）卷积部分**",id:"1卷积部分",depth:4},{text:"**（2）展平操作**",id:"2展平操作",depth:4},{text:"**（3）全连接部分**",id:"3全连接部分",depth:4}],title:"典型架构卷积神经网络的参数恢复",headingTitle:"典型架构卷积神经网络的参数恢复",frontmatter:{}}}}]);